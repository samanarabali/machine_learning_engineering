# Project 3: Understanding User Behavior

For this project, we as data scientists at a game development company will be developing a simple pipeline for communicating between gamers and a mobile app. Our latest mobile game has a few events we are interested in tracking: `buy a wooden sword`, `buy a wooden shield`, `upgrade to a metal sword`, `upgrade to a metal shield`, `sell_a_sword`, `sell_a_shield`, `ditch_a_sword`, and `ditch_a_shield`. The game users perform some activities that generate events. Those events are being communicated in the form of data messages across a secure and reliable HTTP web. We will use a web-based Flask module that performs all the jobs in the background to send and receive web-based messages generated by kafka. Also, all the gamer activities or events will be stored in the Hadoop file system and loaded in Spark and Presto for some analyses.


Next step we create a docker-compose.yml with 6 containers: zookeeper, kafka, mids, spark, presto and cloudera.


### Flask Implementation and Gameplay:

The purpose of the game is for users to trade weapons in a medieval setting. We defined several events in game_api.py, each of which the user can execute from the command line using curl or Apache Bench and calling our API routes. These events include purchasing a wooden sword or shield, upgrading to a metal sword/shield, selling a metal sword/shield, and ditching a wooden sword/shield. As the name describes, each of these events passes 4 key parameters: user, event_type, item, and item_type.

Additionally, we defined a default API route as an initial test function for the user to determine if they can interact with the game. This route includes DELETE functionality on top of the standard GET function.

A python file game_api.py is used to call the flask module to run. All messages/events that users generate from the web (described above) will be fed into Kafka. To enable this functionality, we import the KafkaProducer library in our game_api.py script. Kafka will then publish those events in json format. The second python file is filtered_transform_write.py which does the job of extracting kafka messages, encoding them into 'string' format, transforming them into json, and saving into HDFS.

To begin the game, spin up the docker-compose in detached mode using -d

```
docker-compose up -d
```

check if all the services are running correctly.

```
docker-compose ps
```

Check if the Hadoop HDFS has any existing files.

```
docker-compose exec cloudera hadoop fs -ls /tmp/
```

The output is shown below:

```
Found 2 items
drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn
drwx-wx-wx   - root   supergroup          0 2021-03-28 19:11 /tmp/hive
```

#### Kafka first step -- Create a Kafka Topic

create a topic called `events` with partition 1, replication-factor 1, and use zookeeper to manage the process. We used the kafka-topics utility to do this.

```
docker-compose exec kafka \
  kafka-topics \
    --create \
    --topic events \
    --partitions 1 \
    --replication-factor 1 \
    --if-not-exists \
    --zookeeper zookeeper:32181
```

The output shows that the topic was created.

```
Created topic events.
```
Then checked the topic using kafka-topics and --describe:

```
docker-compose exec kafka \
  kafka-topics \
  --describe \
  --topic events \
  --zookeeper zookeeper:32181
```

```
Topic: events   PartitionCount: 1       ReplicationFactor: 1    Configs: 
        Topic: events   Partition: 0    Leader: 1       Replicas: 1     Isr: 1
```

#### Kafka second step -- Produce messages in Kafka

Note that the Kafka messages will be from gamers web activities. We need to open two terminals since there are two ends in this setup: one terminal for the gamer and one terminal for Flask.

We first run the Flask in the terminal dedicated to Flask window in order to initiate a web-based application game in this case. A gamer will execute game activites on the web browser and the events like "purchase a sword" action will be executed from the gamer terminal.

- Run python flask in the Flask Terminal

Using the following command we are running the FlaskApp (`game_api.py`) from my project 3 folder "w205/project-3-<your folder>/"

```
docker-compose exec mids \
  env FLASK_APP=/w205/project-3-<your folder>/game_api.py \
  flask run --host 0.0.0.0
```

- Web-app: Gamer Activity in Gamer Terminal 

Open a new terminal and change directory to the location where the project is located. Here we are testing events created by a Mobile app that are hitting the browser port 5000. We use the curl tool 

Test the default message:

```
docker-compose exec mids curl http://localhost:5000/
```

The output is:

```
This is the default response!
```

Test the purchasing item message:

```
docker-compose exec mids curl http://localhost:5000/purchase_a_wooden_shield
```

The output is:

```
Wooden Shield Purchased!
```

To create many events we are using Apache Bench (ab). Actually, Apache Bench allows us to replicate the messages. Since we have a few more game features, we might want to generate multiple messages. Below are a series of Apache Bench commands to replicate events with different replication number `n` 

```
docker-compose exec mids ab -n 15 -H "Host: user1.comcast.com" http://localhost:5000/
docker-compose exec mids ab -n 15 -H "Host: user1.comcast.com" http://localhost:5000/purchase_a_wooden_sword
docker-compose exec mids ab -n 12 -H "Host: user2.att.com" http://localhost:5000/purchase_a_wooden_shield
docker-compose exec mids ab -n 12 -H "Host: user3.verizon.com" http://localhost:5000/purchase_a_wooden_sword
docker-compose exec mids ab -n 6 -H "Host: user3.verizon.com" http://localhost:5000/upgrade_to_metal_sword
docker-compose exec mids ab -n 4 -H "Host: user1.comcast.com" http://localhost:5000/upgrade_to_metal_sword
docker-compose exec mids ab -n 4 -H "Host: user1.comcast.com" http://localhost:5000/ditch_a_wooden_sword
docker-compose exec mids ab -n 4 -H "Host: user2.att.com" http://localhost:5000/upgrade_to_metal_shield
docker-compose exec mids ab -n 3 -H "Host: user3.verizon.com" http://localhost:5000/sell_a_metal_sword
docker-compose exec mids ab -n 3 -H "Host: user2.att.com" http://localhost:5000/sell_a_metal_shield
```

Some samples of events created by AB and captured in the Flask terminal are shown below:

```
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:24:47] "GET / HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:25:00] "GET /purchase_a_wooden_sword HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:25:00] "GET /purchase_a_wooden_sword HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:25:00] "GET /purchase_a_wooden_sword HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:25:00] "GET /purchase_a_wooden_sword HTTP/1.0" 200 -
127.0.0.1 - - [12/Apr/2021 02:25:00] "GET /purchase_a_wooden_sword HTTP/1.0" 200 -
```

#### Kafka third step -- Consume messages in Kafka (read from kafka)

Now we consume Kafka message at the backend. Note that we added `KafkaProducer` to our `game_api.py` along with `log_to_kafka()` so that the events we created from the web-app hitting our the browser port 5000 earlier get published to the Kafka topic. We use another terminal window to read events from kafka topic since our Kafka topic `events` has some events already published. We can use kafka-console-consumer to consume messages from a kafka topic. We can also use kafkacat to consume messages. Below, we run the kafkacat command in consumer mode to read events from our kafka topic `events`:

```
docker-compose exec mids bash -c "kafkacat -C -b kafka:29092 -t events -o beginning -e"
```
The first 20 lines of the result are shown below. This includes the first two events created by 'curl' plus the events created by Apache Bench.
 
```
{"Host": "localhost:5000", "event_type": "default", "Accept": "*/*", "User-Agent": "curl/7.47.0"}
{"Host": "localhost:5000", "event_type": "purchase_wooden_shield", "Accept": "*/*", "User-Agent": "curl/7.47.0"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "default", "item_type": null, "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": null, "Host": "user1.comcast.com"}
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": "sword", "Host": "user1.comcast.com"}
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": "sword", "Host": "user1.comcast.com"}
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": "sword", "Host": "user1.comcast.com"}
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": "sword", "Host": "user1.comcast.com"}
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "ApacheBench/2.3", "item": "sword", "Host": "user1.comcast.com"}
```

- Pipelining kafka messages into HDFS using Spark-Submit

Now let us run a spark shell.

```
docker-compose exec spark pyspark
```
Read events from kafka topic events:

```
raw_events = spark \
  .read \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "kafka:29092") \
  .option("subscribe","events") \
  .option("startingOffsets", "earliest") \
  .option("endingOffsets", "latest") \
  .load() 
```

Let us explore our events in spark envirenment. We need to `import json` before using Lambda structure and also use `raw_events.cache()` to cut down the warning messages. Extract the event, show it and show the schema.

```
import json
raw_events.cache()
events = raw_events.select(raw_events.value.cast('string'))
extracted_events = events.rdd.map(lambda x: json.loads(x.value)).toDF()
extracted_events.show()
extracted_events.printSchema()
```

The following result shows the top 20 events that we capture and stored. The event_type, item and item_type that we defined in our `game_api.py` are also added.

```
+------+-----------------+---------------+----------+-----+---------+
|Accept|             Host|     User-Agent|event_type| item|item_type|
+------+-----------------+---------------+----------+-----+---------+
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|   default| null|     null|
|   */*|user1.comcast.com|ApacheBench/2.3|  purchase|sword|     wood|
|   */*|user1.comcast.com|ApacheBench/2.3|  purchase|sword|     wood|
|   */*|user1.comcast.com|ApacheBench/2.3|  purchase|sword|     wood|
|   */*|user1.comcast.com|ApacheBench/2.3|  purchase|sword|     wood|
|   */*|user1.comcast.com|ApacheBench/2.3|  purchase|sword|     wood|
+------+-----------------+---------------+----------+-----+---------+
only showing top 20 rows
```

```
root
 |-- Accept: string (nullable = true)
 |-- Host: string (nullable = true)
 |-- User-Agent: string (nullable = true)
 |-- event_type: string (nullable = true)
 |-- item: string (nullable = true)
 |-- item_type: string (nullable = true)
```

Capture our pyspark code in a file this time using spark-submit. 

Separating events and storing them in Hadoop HDFS. Our `separate_events.py` file basically extract, transform, separate the events and saved them in separate parquet files as well as one parquet for all events. 

```
docker-compose exec spark \
  spark-submit \
  /w205/project-3-<your folder>/separate_events.py
```
  
Now let us see how the separated events are saved in the HDFS:

```
docker-compose exec cloudera hadoop fs -ls /tmp/
```

The output is shown below. As shown, the events are separated by event_type and saved in the corresponding folder. We are also storing all events in a separate parquet file for further analysis. 6 folders are created which are the default_hits, purchases, ditch, sales, upgrades and all_events.

```
Found 8 items
drwxr-xr-x   - root   supergroup          0 2021-04-12 01:54 /tmp/all_events
drwxr-xr-x   - root   supergroup          0 2021-04-12 01:55 /tmp/default_hits
drwxr-xr-x   - root   supergroup          0 2021-04-12 01:55 /tmp/ditch
drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn
drwx-wx-wx   - root   supergroup          0 2021-04-12 01:08 /tmp/hive
drwxr-xr-x   - root   supergroup          0 2021-04-12 01:55 /tmp/purchases
drwxr-xr-x   - root   supergroup          0 2021-04-12 01:55 /tmp/sales
drwxr-xr-x   - root   supergroup          0 2021-04-12 01:55 /tmp/upgrades
```

Now we check the content of one of the folders for a sanity check in the pyspark environment. We check the 'purchases' folder. It should only include all the purchases. As shown below, the event_type of all messages is 'purchase' which means the event separation actually worked very well!

```
docker-compose exec spark pyspark
my_extracted_events = sqlContext.read.parquet('/tmp/purchases')
my_extracted_events.show()
```

As shown below all purchance events are saved in the purchase parquet file.

```
+------+-------------+-----------------+---------------+----------+------+---------+--------------------+
|Accept|Cache-Control|             Host|     User-Agent|event_type|  item|item_type|           timestamp|
+------+-------------+-----------------+---------------+----------+------+---------+--------------------+
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|user1.comcast.com|ApacheBench/2.3|  purchase| sword|     wood|2021-04-12 01:24:...|
|   */*|     no-cache|    user2.att.com|ApacheBench/2.3|  purchase|shield|     wood|2021-04-12 01:35:...|
|   */*|     no-cache|    user2.att.com|ApacheBench/2.3|  purchase|shield|     wood|2021-04-12 01:35:...|
|   */*|     no-cache|    user2.att.com|ApacheBench/2.3|  purchase|shield|     wood|2021-04-12 01:35:...|
|   */*|     no-cache|    user2.att.com|ApacheBench/2.3|  purchase|shield|     wood|2021-04-12 01:35:...|
|   */*|     no-cache|    user2.att.com|ApacheBench/2.3|  purchase|shield|     wood|2021-04-12 01:35:...|
+------+-------------+-----------------+---------------+----------+------+---------+--------------------+
only showing top 20 rows
```


#### Running Spark in Jupyter Notebook

On another attempt we run Spark in Jupyter Notebook. To do so we first modify our the Spark part of our .yml file by exposing the port 7000 and mapping the instance as shown below:   

```
 expose:
   - "7000"   # jupyter notebook
 ports:
   - "7000:7000"  # map instance:service port
```

In another terminal window, we create a symbolic link for our mounted directory to spark environment.

```
docker-compose exec spark bash
```

Create a symbolic link from the spark directory to /w205:

```
ln -s /w205 w205
exit
```

The following command is going to create a token to run Jupyter Notebook:

```
docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 7000 --ip 0.0.0.0 --allow-root --notebook-dir=/w205/' pyspark
```

Using the token given, we ran the Jupyter notebook and created `HDFS_to_Spark_SQL.ipynb` which includes the commands needed to read the events from HDFS parquet files into a Spark SQL and then convert them to a DataFrame for some analysis. The notebook file is annotated. 

This time we changed our `filtered_writes.py` so that it only filters out the default events and stores all other events in a hdfs parquet file called `tmp/all_events`, so that we can perform some simple analysis in Jupyter Notebook using spark-SQL and/or DataFrame. Below is a sample output of the notebook. The following table shows the user activities groupped by Host, evnt_type, item, item_type and aggregated by frequency of each activity. Please see the notebook for more analysis.  

```
        Host               event_type     item    item_type  frequency
0   user1.comcast.com        ditch        sword     wood        4
1   user1.comcast.com        purchase     sword     wood        15
2   user1.comcast.com        upgrade      sword     metal       4
3   user2.att.com            purchase     shield    wood        12
4   user2.att.com            sell         shield    metal       3
5   user2.att.com            upgrade      shield    metal       4
6   user3.verizon.com        purchase     sword     wood        12
7   user3.verizon.com        sell         sword     metal       3
8   user3.verizon.com        upgrade      sword     metal       6
```

## Method 2:

### Load data to Hive and Query with Presto

For the second method, we will be loading tables to Hive through a single Python script and querying the resulting table using Presto.

- Write tables to hive

```
docker-compose exec spark spark-submit /w205/project-3-asimmeth/write_hive_table.py
```
The command above initiated the Python script write_hive_table.py. The resulting job loads the events from /tmp and creates the table ‘game_events’ from the extracted events. The resulting table can be queried for further analysis

- Confirm file is created

```
docker-compose exec cloudera hadoop fs -ls /tmp/
```

- Initiate Presto query engine

```
docker-compose exec presto presto --server presto:8080 --catalog hive --schema default
```

The command above initiates the Presto query engine that will allow us to query the hive server and the game_events table that we created. Presto handles a wider range of range of sql syntax that can aid in our analysis.

```
describe game_events;
```
The schema output shows that we sucessfully loaded the table and the columns we can query. Since all events types are included in a single table we will have to filter by 'event_type' to anaylze sepcific events

```
   Column   |  Type   | Comment 
------------+---------+---------
 accept     | varchar |         
 host       | varchar |         
 user-agent | varchar |         
 event_type | varchar |         
 item       | varchar |         
 item_type  | varchar |         
 timestamp  | varchar |   
 ```

### Analysis

Using our tables we can run queries to answer potential questions around who is using the game and how they are playing in order to improve the game.

```
select DISTINCT(host) from game_events;
```
Using the query above, we can identify the hosts each event is assigned too. This will tell us who is using the game, how they are playing (events stored), and the total number of events.

```
       host        
-------------------
 localhost:5000    
 user1.comcast.com 
 user2.att.com     
 user3.verizon.com 
 ```

Based on our created dataset we identify 3 hosts in addition to the default host. We can drill down into 'user1.comcast.com' to identify the events they have played by using the following query.
 

- run additional queries and describe data

```
select count(*) from game_events where host='user1.comcast.com';
23 events

select distinct(event_type) from game_events where host='user1.comcast.com';

event_type 
------------
 default    
 upgrade    
 ditch 
 
 select distinctitem, item_type from game_events where host='user1.comcast.com';
 
  item  | item_type 
-------+-----------
 NULL  | NULL      
 sword | metal     
 sword | wood  
```
The results of our queries show the number of events a user has triggered, the event_types, and the items he has acquired. These results can be utilized to identify patterns among users and improve the game based on user activity.


#### Testing some optional features

In this section, we will slightly enhance the API to use additional http verbs like `GET` and `DELETE`. We made changes to 'game_api.py' that enable the Web app to either create get or delete events. However, deleting the user is not actually implemented. This is a just simple implementation that demonstrates the ability to use those keywords to make decisions whether to create appropriate events. We implemented it on a very simplified version of `game_api.py`, but it could be easily extended. A modified version of `@app.route` by adding `methods = ['GET','DELETE']` is shown bellow. A simple if-loop then checks if the event is `GET` or `DELETE` and takes appropriate actions.

```
@app.route("/purchase_a_wooden_sword", methods = ['GET','DELETE'])
def purchase_a_sword():
    if request.method == 'DELETE':
        delete_purchase_sword_event = {'event_type': 'delete',
                                       'item': 'sword',
                                       'item_type': 'wood'}
        log_to_kafka(events_topic, delete_purchase_sword_event)
        return "\n Wooden Sword Purchase Deleted!\n"        
    else:
        purchase_sword_event = {'event_type': 'purchase',
                                'item': 'sword',
                                'item_type': 'wood'}
        log_to_kafka(events_topic, purchase_sword_event)
        return "\n Wooden Sword Purchased!\n"
```

This modification has been done to our `game_api.py`. With this modification, now we need to pass a new parameter when testing the flask app using the `curl` method. We run the flask in one terminal and create `curl` commands from another CLI terminal. Below are the samples of `curl` function with `-X` that allows to pass `GET` and `DELETE` along with http. 

A Sample of `curl` with a `GET` key:
```
docker-compose exec mids curl -X GET  http://localhost:5000/purchase_a_wooden_sword
```

The output of the flask app is shown below. Note that by default `curl` creates a GET event if `-X` is not used, so all earlier `curl` calls were working because of that default setting.

```
Wooden Sword Purchased!
```

A Sample of `curl` with a `DELETE` key:

```
docker-compose exec mids curl -X DELETE  http://localhost:5000/purchase_a_wooden_sword
```
The output of the flask app:

```
Wooden Sword Purchase Deleted!
```

A few more commands:

```
docker-compose exec mids curl -X GET  http://localhost:5000/purchase_a_wooden_sword
docker-compose exec mids curl -X GET  http://localhost:5000/upgrade_to_metal_sword
docker-compose exec mids curl -X DELETE  http://localhost:5000/sell_a_metal_sword
docker-compose exec mids curl -X DELETE  http://localhost:5000/ditch_a_wooden_shield
docker-compose exec mids curl -X DELETE  http://localhost:5000/ditch_a_wooden_sword
```

Now let's check the Kafka topic `events`:

```
docker-compose exec mids bash -c "kafkacat -C -b kafka:29092 -t events -o beginning -e"
```
The output is shown below. As expected the second event has an event_type of `DELETE`. This flag can be easily used in our `separate_events.py` to remove the item which raised the `DELETE` event.

```
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "sword", "Host": "localhost:5000"}
{"event_type": "delete", "item_type": "wood", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "sword", "Host": "localhost:5000"}
{"event_type": "purchase", "item_type": "wood", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "sword", "Host": "localhost:5000"}
{"event_type": "upgrade", "item_type": "metal", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "sword", "Host": "localhost:5000"}
{"event_type": "delete", "item_type": "metal", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "sword", "Host": "localhost:5000"}
{"event_type": "delete", "item_type": "wood", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "shield", "Host": "localhost:5000"}
{"event_type": "delete", "item_type": "wood", "Accept": "*/*", "User-Agent": "curl/7.47.0", "item": "sword", "Host": "localhost:5000"}
% Reached end of topic events [0] at offset 7: exiting
```


In conclusion, we have developed a simple pipeline for communicating between gamers and a mobile app. The game users perform some activities that generate events and those events are being communicated in the form of data messages across a HTTP web. We have used a web-based Flask module `game_api.py` that performs all the jobs in the background to send and receive web-based messages consumed by kafka. Also, all the gamer activities or events have been extacted, transformed, separated and stored in the Hadoop file system using `separate_events.py` and loaded in Spark and Presto for some analyses using `write_hive_table.py` and `HDFS_to_Spark_SQL.ipynb`. 


List of files in the repo:

- HDFS_to_Spark_SQL.ipynb  # Jupyter Notebook
- P3_README.md # description of the project
- docker-compose.yml  # docker-compose setup
- game_api.py # our Flask app
- separate_events.py  # a python file to extract, transform, separate, filter events and write into parquet files
- write_hive_table.py  # analysis in Presto